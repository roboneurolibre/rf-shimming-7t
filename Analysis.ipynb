{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "459f60d5",
   "metadata": {},
   "source": [
    "# Analysis code for the paper \"B1+ shimming in the cervical spinal cord at 7T\"\n",
    "\n",
    "## Assumed input data:\n",
    "This code is written with the assumption that, for every subject, the data is organized as:\n",
    "_Subject_\n",
    "__RF shim directory (Seven directories per subject)\n",
    "    GRE scan\n",
    "    TFL_B1map scan\n",
    "__MPRAGE directory\n",
    "\n",
    "## Analysis procedure:\n",
    "\n",
    "For each subject:\n",
    "-->For each RF shim directory\n",
    "\n",
    "---->Analyze GRE scans by:\n",
    "----->Segment the SC from the GRE scan\n",
    "----->Extract the signal intensity within this mask from the GRE scans\n",
    "\n",
    "---->Analyze B1+ maps by:\n",
    "----->Coregistering the anatomical image of the TFL_B1map scan to the corresponding GRE scan (noshim to noshim, CVred to CVred, etc)\n",
    "----->Warp the previously created mask to the TFL_B1map space\n",
    "----->Convert the B1+ maps to nT/V units\n",
    "----->Extract the B1+ value within the coregistered mask\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f78c5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Necessary imports and helper function\n",
    "\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import subprocess\n",
    "import glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "from tabulate import tabulate\n",
    "import nibabel as nib\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f4227bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "import os\n",
    "import fnmatch\n",
    "\n",
    "\n",
    "def count_visible_subdirectories(folder_path):\n",
    "    subdirectories = [subdir for subdir in os.listdir(folder_path) if os.path.isdir(os.path.join(folder_path, subdir)) and not subdir.startswith('.')]\n",
    "    return len(subdirectories)\n",
    "\n",
    "def named_subdir(folder_path,name):\n",
    "    for dir_name in os.listdir(folder_path):\n",
    "        if dir_name.lower() == name:\n",
    "            return os.path.join(folder_path, dir_name)\n",
    "    return None  # If no matching directory is found\n",
    "\n",
    "\n",
    "# Helper function to find a specific string in a specific location in a  JSON and return the matching nifti filename\n",
    "def find_matching_nii_json_pairs(directory_path, keyword, keywordlocation):\n",
    "    nii_filename = []\n",
    "\n",
    "    for filename in os.listdir(directory_path):\n",
    "        if filename.endswith(\".json\"):\n",
    "            json_file_path = os.path.join(directory_path, filename)\n",
    "            nii_file_path = os.path.join(directory_path, filename.replace(\".json\", \".nii.gz\"))\n",
    "\n",
    "            if os.path.exists(nii_file_path):\n",
    "                with open(json_file_path, 'r') as json_file:\n",
    "                    data = json.load(json_file)\n",
    "                    if keywordlocation in data and keyword in data[keywordlocation]:\n",
    "                        nii_filename.append(nii_file_path)\n",
    "\n",
    "    if not nii_filename:\n",
    "        raise ValueError(\"No matching nii.gz files found.\")\n",
    "\n",
    "    return nii_filename\n",
    "\n",
    "def fetch_file_via_name(directory_path,expression):\n",
    "    directory_abspath=os.path.abspath(directory_path)\n",
    "    files = [os.path.join(directory_abspath, file) for file in os.listdir(directory_abspath) if os.path.isfile(os.path.join(directory_abspath, file)) and not file.startswith('.')]\n",
    "    filtered_files = [file for file in files if fnmatch.fnmatch(file, expression)]\n",
    "    return filtered_files\n",
    "    \n",
    "# Helper function for subprocesses\n",
    "def run_subprocess(cmd):\n",
    "    \"\"\"Wrapper for ``subprocess.run()`` that enables to input ``cmd`` as a full string (easier for debugging).\n",
    "    Args:\n",
    "        cmd (string): full command to be run on the command line\n",
    "    \"\"\"\n",
    "    try:\n",
    "        subprocess.run(\n",
    "            cmd.split(' '),\n",
    "            text=True,\n",
    "            check=True,\n",
    "            stdout=subprocess.PIPE,\n",
    "            stderr=subprocess.STDOUT,\n",
    "        )\n",
    "    except subprocess.CalledProcessError as err:\n",
    "        print(f\"Return code: {err.returncode}\")\n",
    "        print(\"Output:\", err.output)\n",
    "        raise\n",
    "\n",
    "def GRE_QA_visualizer(datafile,overlayfile):\n",
    "\n",
    "    datafile_data=nib.load(datafile)\n",
    "    datafile_data=datafile_data.get_fdata()\n",
    "    \n",
    "    overlayfile_data=nib.load(overlayfile)\n",
    "    overlayfile_data=overlayfile_data.get_fdata()\n",
    "    overlayfile_data = np.ma.masked_where(overlayfile_data < 0.3, overlayfile_data)\n",
    "    \n",
    "    centralslice=(np.floor(datafile_data.shape[0]/2)).astype(int)\n",
    "    fig, ax = plt.subplots()\n",
    "    plt.axis('off')\n",
    "    img1 = ax.imshow(np.rot90(datafile_data[centralslice,:,:]), cmap=plt.cm.gray)\n",
    "    ax.imshow(np.rot90(overlayfile_data[centralslice,:,:]), cmap=plt.cm.autumn, interpolation='none', alpha=0.5)\n",
    "    plt.show()\n",
    "\n",
    "def create_QA_and_Processing_dirs(indir):\n",
    "    QAdir=os.path.join(indir,'QA')\n",
    "    if os.path.isdir(QAdir):\n",
    "        print('The QA directory already exists! Are you rerunnig the script?')\n",
    "    else:\n",
    "        os.makedirs(QAdir)\n",
    "    Processingdir=os.path.join(indir,'Processing')\n",
    "    if os.path.isdir(Processingdir):\n",
    "        print('The Processing directory already exists! Are you rerunnig the script?')\n",
    "    else:\n",
    "        os.makedirs(Processingdir)\n",
    "    return QAdir, Processingdir\n",
    "\n",
    "def create_QA_and_Processing_dirs_B1(indir):\n",
    "    QAdir=os.path.join(indir,'QA_B1')\n",
    "    if os.path.isdir(QAdir):\n",
    "        print('The QA directory already exists! Are you rerunnig the script?')\n",
    "    else:\n",
    "        os.makedirs(QAdir)\n",
    "    Processingdir=os.path.join(indir,'Processing_B1')\n",
    "    if os.path.isdir(Processingdir):\n",
    "        print('The Processing directory already exists! Are you rerunnig the script?')\n",
    "    else:\n",
    "        os.makedirs(Processingdir)\n",
    "    return QAdir, Processingdir\n",
    "\n",
    "def create_QA_and_Processing_dirs_MPRAGE(indir):\n",
    "    QAdir=os.path.join(indir,'QA_MPRAGE')\n",
    "    if os.path.isdir(QAdir):\n",
    "        print('The QA directory already exists! Are you rerunnig the script?')\n",
    "    else:\n",
    "        os.makedirs(QAdir)\n",
    "    Processingdir=os.path.join(indir,'Processing_MPRAGE')\n",
    "    if os.path.isdir(Processingdir):\n",
    "        print('The Processing directory already exists! Are you rerunnig the script?')\n",
    "    else:\n",
    "        os.makedirs(Processingdir)\n",
    "    return QAdir, Processingdir\n",
    "\n",
    "def check_number_of_subdirs(indir,number):\n",
    "    if not count_visible_subdirectories(indir) ==number:\n",
    "        print('Unexpected number of subdirectories. Please check your data and rerun!')\n",
    "        exit()\n",
    "\n",
    "def check_number_of_GRE_scans_distcorr(filenames):\n",
    "    if len(filenames)==1 or len(filenames)==2:\n",
    "        distcorr_filename=filenames[-1] #this is the one with distortion correction\n",
    "        return distcorr_filename\n",
    "    else:\n",
    "        print('Unexpected number of NIFTI files found. Please inspect data and rerun!')\n",
    "        exit() \n",
    "\n",
    "def check_number_of_GRE_scans_nodistcorr(filenames):\n",
    "    if len(filenames)==1 or len(filenames)==2:\n",
    "        nodistcorr_filename=filenames[0] #this is the one without distortion correction\n",
    "        return nodistcorr_filename\n",
    "    else:\n",
    "        print('Unexpected number of NIFTI files found. Please inspect data and rerun!')\n",
    "        exit()\n",
    "\n",
    "# Extracts the Reference Voltage from a json file\n",
    "def extract_tx_ref_amp(json_file): #this function will extract the TxRefAmp value of the json file, which is\n",
    "    # the reference voltage\n",
    "    with open(json_file, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "        return data.get(\"TxRefAmp\", \"N/A\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d7e57cb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cpu\n",
      "Requirement already satisfied: dynamic_network_architectures==0.2 in /Users/danielpapp/miniconda3/lib/python3.10/site-packages (from -r /Users/danielpapp/contrast-agnostic-softseg-spinalcord-2.0/monai/requirements_inference_cpu.txt (line 1)) (0.2)\n",
      "Requirement already satisfied: joblib==1.3.0 in /Users/danielpapp/miniconda3/lib/python3.10/site-packages (from -r /Users/danielpapp/contrast-agnostic-softseg-spinalcord-2.0/monai/requirements_inference_cpu.txt (line 2)) (1.3.0)\n",
      "Requirement already satisfied: loguru==0.7.0 in /Users/danielpapp/miniconda3/lib/python3.10/site-packages (from -r /Users/danielpapp/contrast-agnostic-softseg-spinalcord-2.0/monai/requirements_inference_cpu.txt (line 3)) (0.7.0)\n",
      "Requirement already satisfied: monai[nibabel]==1.3.0 in /Users/danielpapp/miniconda3/lib/python3.10/site-packages (from -r /Users/danielpapp/contrast-agnostic-softseg-spinalcord-2.0/monai/requirements_inference_cpu.txt (line 4)) (1.3.0)\n",
      "Requirement already satisfied: scipy==1.11.2 in /Users/danielpapp/miniconda3/lib/python3.10/site-packages (from -r /Users/danielpapp/contrast-agnostic-softseg-spinalcord-2.0/monai/requirements_inference_cpu.txt (line 5)) (1.11.2)\n",
      "Requirement already satisfied: numpy==1.24.4 in /Users/danielpapp/miniconda3/lib/python3.10/site-packages (from -r /Users/danielpapp/contrast-agnostic-softseg-spinalcord-2.0/monai/requirements_inference_cpu.txt (line 6)) (1.24.4)\n",
      "Requirement already satisfied: torch==2.0.0 in /Users/danielpapp/miniconda3/lib/python3.10/site-packages (from -r /Users/danielpapp/contrast-agnostic-softseg-spinalcord-2.0/monai/requirements_inference_cpu.txt (line 7)) (2.0.0)\n",
      "Requirement already satisfied: nibabel in /Users/danielpapp/miniconda3/lib/python3.10/site-packages (from monai[nibabel]==1.3.0->-r /Users/danielpapp/contrast-agnostic-softseg-spinalcord-2.0/monai/requirements_inference_cpu.txt (line 4)) (5.1.0)\n",
      "Requirement already satisfied: jinja2 in /Users/danielpapp/miniconda3/lib/python3.10/site-packages (from torch==2.0.0->-r /Users/danielpapp/contrast-agnostic-softseg-spinalcord-2.0/monai/requirements_inference_cpu.txt (line 7)) (3.1.2)\n",
      "Requirement already satisfied: networkx in /Users/danielpapp/miniconda3/lib/python3.10/site-packages (from torch==2.0.0->-r /Users/danielpapp/contrast-agnostic-softseg-spinalcord-2.0/monai/requirements_inference_cpu.txt (line 7)) (3.2.1)\n",
      "Requirement already satisfied: sympy in /Users/danielpapp/miniconda3/lib/python3.10/site-packages (from torch==2.0.0->-r /Users/danielpapp/contrast-agnostic-softseg-spinalcord-2.0/monai/requirements_inference_cpu.txt (line 7)) (1.12)\n",
      "Requirement already satisfied: filelock in /Users/danielpapp/miniconda3/lib/python3.10/site-packages (from torch==2.0.0->-r /Users/danielpapp/contrast-agnostic-softseg-spinalcord-2.0/monai/requirements_inference_cpu.txt (line 7)) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in /Users/danielpapp/miniconda3/lib/python3.10/site-packages (from torch==2.0.0->-r /Users/danielpapp/contrast-agnostic-softseg-spinalcord-2.0/monai/requirements_inference_cpu.txt (line 7)) (4.9.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/danielpapp/miniconda3/lib/python3.10/site-packages (from jinja2->torch==2.0.0->-r /Users/danielpapp/contrast-agnostic-softseg-spinalcord-2.0/monai/requirements_inference_cpu.txt (line 7)) (2.1.2)\n",
      "Requirement already satisfied: packaging>=17 in /Users/danielpapp/miniconda3/lib/python3.10/site-packages (from nibabel->monai[nibabel]==1.3.0->-r /Users/danielpapp/contrast-agnostic-softseg-spinalcord-2.0/monai/requirements_inference_cpu.txt (line 4)) (23.0)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/danielpapp/miniconda3/lib/python3.10/site-packages (from sympy->torch==2.0.0->-r /Users/danielpapp/contrast-agnostic-softseg-spinalcord-2.0/monai/requirements_inference_cpu.txt (line 7)) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "## Setup for contrast agnostic soft segmentation\n",
    "## REQUIRES THE USE OF THE VENV_MONAI SET UP BELOW\n",
    "## Requires the notebook being run with the venv_monai kernel!\n",
    "#1 Install the necessary tools as described here:\n",
    "# https://github.com/sct-pipeline/contrast-agnostic-softseg-spinalcord/tree/main/monai\n",
    "#2, Download the inference dataset (model 2023 09 18 zip) from here:\n",
    "# https://github.com/sct-pipeline/contrast-agnostic-softseg-spinalcord/releases/tag/v2.0\n",
    "# And unzip it into a folder\n",
    "monai_checkpoint='/Users/danielpapp/model_2023-09-18/'\n",
    "\n",
    "#and create an alias for calling it, otherwise its going to be a lot of typing\n",
    "monai_command='python /Users/danielpapp/contrast-agnostic-softseg-spinalcord-2.0/monai/run_inference_single_image.py'\n",
    "\n",
    "# reinstalling requrements, just in case\n",
    "!pip install -r /Users/danielpapp/contrast-agnostic-softseg-spinalcord-2.0/monai/requirements_inference_cpu.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b58dace",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GRE_segment_monai(subjectpath, indirname):\n",
    "    #Define the GRE scan directory the GRE scans\n",
    "    GRE_dir=os.path.join(subjectpath,'GRE_B1')   \n",
    "\n",
    "    #We start by finding the directory, and creating the subdirectories for processing and QA\n",
    "    dirname_shimcase=named_subdir(GRE_dir,indirname)\n",
    "    [QAdir, Processingdir]=create_QA_and_Processing_dirs(dirname_shimcase)\n",
    "    \n",
    "    #We fetch the GRE scans and check that there are only one or two (Wih and without distortion correction)\n",
    "    #Changed to use the one WITHOUT distortion correction, as this will be needed for B1 processing\n",
    "    GRE_filenames=fetch_file_via_name(dirname_shimcase,'*gre*nii.gz')\n",
    "    GRE_shimfile_nodistcorr=check_number_of_GRE_scans_nodistcorr(GRE_filenames)\n",
    "    \n",
    "    #Sometimes there is a 0 -0 discrepancy between the QFORM and SFORM for MGH data. To fix this, we use the \n",
    "    # 'set-qform-to-sfrom' option of sct_image\n",
    "    run_subprocess(f\"sct_image -i {GRE_shimfile_nodistcorr} -set-qform-to-sform\")\n",
    "    \n",
    "    #We can now segment this file\n",
    "    run_subprocess(f\"{monai_command} --path-img {GRE_shimfile_nodistcorr} --path-out {Processingdir} --chkp-path {monai_checkpoint}\")\n",
    "    \n",
    "    #And turn the soft segmentation into a hard one, so as to not mix evaluating the segmentation and the RF shimming\n",
    "    GRE_shim_monaisegfilename=fetch_file_via_name(Processingdir,'*gre*_pred.nii.gz')\n",
    "    GRE_shim_monaisegfilename=GRE_shim_monaisegfilename[0]\n",
    "    GRE_shim_monaisegfilename_hard=GRE_shim_monaisegfilename.split('.')[0]+'_hard.nii.gz'\n",
    "    run_subprocess(f\"sct_maths -i {GRE_shim_monaisegfilename} -o {GRE_shim_monaisegfilename_hard} -bin 0.5\")\n",
    "    \n",
    "    #And then we need to do manual checking\n",
    "    #And extract the signal intensity    \n",
    "    #GRE_shim_segfilename=fetch_file_via_name(Processingdir,'*gre*_seg.nii.gz')\n",
    "    #GRE_shim_segfilename=GRE_shim_segfilename[0]\n",
    "    #GRE_shim_CSVfile=os.path.join(Processingdir,GRE_shimfile_distcorr.split('/')[-1].split('.')[0]+'_sigint.csv')\n",
    "    #run_subprocess(f\"sct_extract_metric -i {GRE_shimfile_distcorr} -f {GRE_shim_segfilename} -o {GRE_shim_CSVfile} -perslice 1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a5313e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GRE_extract_monai(subjectpath,indirname):\n",
    "    GRE_dir=os.path.join(subjectpath,'GRE_B1')   \n",
    "\n",
    "    #We start by finding the directory, and creating the subdirectories for processing and QA\n",
    "    dirname_shimcase=named_subdir(GRE_dir,indirname)\n",
    "    [QAdir, Processingdir]=create_QA_and_Processing_dirs(dirname_shimcase)\n",
    "    \n",
    "    #We fetch the GRE scans and check that there are only one or two (Wih and without distortion correction)\n",
    "    #Changed to use the one WITHOUT distortion correction, as this will be needed for B1 processing\n",
    "    GRE_filenames=fetch_file_via_name(dirname_shimcase,'*gre*nii.gz')\n",
    "    GRE_shimfile_nodistcorr=check_number_of_GRE_scans_nodistcorr(GRE_filenames)\n",
    "    \n",
    "    #And extract the signal intensity\n",
    "    #Check if there is a corrected file\n",
    "    if not fetch_file_via_name(Processingdir,'*gre*_pred_corr.nii.gz'):\n",
    "        GRE_shim_monaisegfilename=fetch_file_via_name(Processingdir,'*gre*_pred_hard.nii.gz')\n",
    "    else:\n",
    "        GRE_shim_monaisegfilename=fetch_file_via_name(Processingdir,'*gre*_pred_hard_corr.nii.gz')\n",
    "    GRE_shim_monaisegfilename=GRE_shim_monaisegfilename[0]\n",
    "    \n",
    "    GRE_shim_CSVfile=os.path.join(Processingdir,GRE_shimfile_nodistcorr.split('/')[-1].split('.')[0]+'_sigint.csv')\n",
    "    run_subprocess(f\"sct_extract_metric -i {GRE_shimfile_nodistcorr} -f {GRE_shim_monaisegfilename} -o {GRE_shim_CSVfile} -perslice 1\")\n",
    "\n",
    "\n",
    "    #And extract the signal intensity    \n",
    "    #GRE_shim_segfilename=fetch_file_via_name(Processingdir,'*gre*_seg.nii.gz')\n",
    "    #GRE_shim_segfilename=GRE_shim_segfilename[0]\n",
    "    #GRE_shim_CSVfile=os.path.join(Processingdir,GRE_shimfile_distcorr.split('/')[-1].split('.')[0]+'_sigint.csv')\n",
    "    #run_subprocess(f\"sct_extract_metric -i {GRE_shimfile_distcorr} -f {GRE_shim_segfilename} -o {GRE_shim_CSVfile} -perslice 1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "de904d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def B1_map_process_monai(subjectpath,indirname):\n",
    "    #Define the GRE scan directory the GRE scans\n",
    "    GRE_dir=os.path.join(subjectpath,'GRE_B1')   \n",
    "\n",
    "    #We start by finding the directory, and creating the subdirectories for processing and QA\n",
    "    dirname_shimcase=named_subdir(GRE_dir,indirname)\n",
    "    [QAdir_B1, Processingdir_B1]=create_QA_and_Processing_dirs_B1(dirname_shimcase)\n",
    "    [QAdir, Processingdir]=create_QA_and_Processing_dirs(dirname_shimcase)\n",
    "   \n",
    "    #Find GRE filenames\n",
    "    GRE_filenames=fetch_file_via_name(dirname_shimcase,'*gre*nii.gz')\n",
    "    GRE_shimfile_nodistcorr=check_number_of_GRE_scans_nodistcorr(GRE_filenames)\n",
    "    \n",
    "    #We fetch the proper segmentation\n",
    "    if not fetch_file_via_name(Processingdir,'*gre*_pred_corr.nii.gz'):\n",
    "        GRE_shim_monaisegfilename=fetch_file_via_name(Processingdir,'*gre*_pred_hard.nii.gz')\n",
    "    else:\n",
    "        GRE_shim_monaisegfilename=fetch_file_via_name(Processingdir,'*gre*_pred_hard_corr.nii.gz')\n",
    "    GRE_shim_monaisegfilename=GRE_shim_monaisegfilename[0]\n",
    "    \n",
    "    #Next we find the TFL_B1maps anatomical and flip angle maps\n",
    "    TFL_anatfile=find_matching_nii_json_pairs(dirname_shimcase, 'anatomical', 'ImageComments')[0]\n",
    "    TFL_FAfile=find_matching_nii_json_pairs(dirname_shimcase, 'angle map', 'ImageComments')[0]\n",
    "\n",
    "    #We then convert the flip angle map into a nT/V map\n",
    "    # Maths from Kyle Gilbert\n",
    "    #GAMMA = 2.675e8\n",
    "    #B1eff_mag = (AcquiredFA ./ RequestedFA) .* (pi ./ (GAMMA .* 1e-3 .* VoltageAtSocket)); % [T/V]\n",
    "    #B1eff_mag = B1eff_mag .* 1e9; % [T/V] to [nT/V]\n",
    "    # The costants sum up to 130.492, so to convert the B1map to nT/V, it has to be divided by 10 (to get it back into units of FA)\n",
    "    # then multiplied by 130.492 and divided by the VoltageAtSocket\n",
    "\n",
    "    TFL_FAfile_nTpV=os.path.join(Processingdir_B1,TFL_FAfile.split('.')[0].split('/')[-1]+'_nTpV.nii.gz')\n",
    "    TFL_FAfile_jsonfile = TFL_FAfile.replace(\".nii.gz\", \".json\")\n",
    "    RefVol = extract_tx_ref_amp(TFL_FAfile_jsonfile)\n",
    "    VoltageAtSocket = RefVol * 10**-0.095\n",
    "    VoltageAtSocket=np.around(VoltageAtSocket, decimals=2)\n",
    "    run_subprocess(f\"sct_maths -i {TFL_FAfile} -div 10 -o {TFL_FAfile_nTpV}\")\n",
    "    run_subprocess(f\"sct_maths -i {TFL_FAfile_nTpV} -div {VoltageAtSocket} -o {TFL_FAfile_nTpV}\")\n",
    "    run_subprocess(f\"sct_maths -i {TFL_FAfile_nTpV} -mul 130.492 -o {TFL_FAfile_nTpV}\")\n",
    "\n",
    "    #Then we coreigster the B1map anatomical to the undistcorrected GRE\n",
    "    warp_anat_2_B1_fname=os.path.join(Processingdir_B1,GRE_shimfile_nodistcorr.split('.')[0].split('/')[-1]+'_warp2B1.nii.gz')\n",
    "    run_subprocess(f\"sct_register_multimodal -i {TFL_anatfile} -d {GRE_shimfile_nodistcorr} -ofolder {Processingdir_B1} -qc {QAdir_B1} -dseg {GRE_shim_monaisegfilename} -owarp {warp_anat_2_B1_fname}\")\n",
    "\n",
    "\n",
    "    #Apply this transformation to the segmentation we just produced\n",
    "    warped_segname=os.path.join(Processingdir_B1,GRE_shim_monaisegfilename.split('.')[0].split('/')[-1]+'_warped.nii.gz')\n",
    "    run_subprocess(f\"sct_apply_transfo -i {GRE_shim_monaisegfilename} -d {TFL_anatfile} -o {warped_segname} -x nn -w {warp_anat_2_B1_fname}\")\n",
    "\n",
    "    #And finally extact the metric\n",
    "    TFL_FAfile_nTpV_CSV=(TFL_FAfile_nTpV.split('.')[0]+'_SC.csv')\n",
    "    run_subprocess(f\"sct_extract_metric -i {TFL_FAfile_nTpV} -f {warped_segname} -o {TFL_FAfile_nTpV_CSV} -perslice 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "11ba2142",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MPRAGE_segment_monai(subjectpath):\n",
    "    #Define the GRE scan directory the GRE scans\n",
    "    MPRAGE_dir=os.path.join(subjectpath,'MPRAGE')   \n",
    "\n",
    "    #We start by finding the directory, and creating the subdirectories for processing and QA\n",
    "    [QAdir, Processingdir]=create_QA_and_Processing_dirs(MPRAGE_dir)\n",
    "    \n",
    "    MPRAGE_filenames_noshim=fetch_file_via_name(MPRAGE_dir,'*mprage*no*nii.gz')\n",
    "    MPRAGE_shimfile_noshim=check_number_of_GRE_scans_distcorr(MPRAGE_filenames_noshim)\n",
    "\n",
    "    MPRAGE_filenames_rfshim=fetch_file_via_name(MPRAGE_dir,'*mprage*CV*nii.gz')\n",
    "    MPRAGE_shimfile_rfshim=check_number_of_GRE_scans_distcorr(MPRAGE_filenames_rfshim)\n",
    "    \n",
    "    #Sometimes there is a 0 -0 discrepancy between the QFORM and SFORM for MGH data. To fix this, we use the \n",
    "    # 'set-qform-to-sfrom' option of sct_image\n",
    "    #run_subprocess(f\"sct_image -i {MPRAGE_shimfile_distcorr} -set-qform-to-sform\")\n",
    "    \n",
    "    #We can now segment this files\n",
    "    run_subprocess(f\"{monai_command} --path-img {MPRAGE_shimfile_noshim} --path-out {Processingdir} --chkp-path {monai_checkpoint}\")\n",
    "    run_subprocess(f\"{monai_command} --path-img {MPRAGE_shimfile_rfshim} --path-out {Processingdir} --chkp-path {monai_checkpoint}\")\n",
    "    \n",
    "    #And turn the soft segmentation into a hard one, so as to not mix evaluating the segmentation and the RF shimming\n",
    "    MPRAGE_shim_monaisegfilename_noshim=fetch_file_via_name(Processingdir,'*mprage*no*_pred.nii.gz')\n",
    "    MPRAGE_shim_monaisegfilename_noshim=MPRAGE_shim_monaisegfilename_noshim[0]\n",
    "    MPRAGE_shim_monaisegfilename_hard_noshim=MPRAGE_shim_monaisegfilename_noshim.split('.')[0]+'_hard.nii.gz'\n",
    "    run_subprocess(f\"sct_maths -i {MPRAGE_shim_monaisegfilename_noshim} -o {MPRAGE_shim_monaisegfilename_hard_noshim} -bin 0.5\")\n",
    "    \n",
    "    #And turn the soft segmentation into a hard one, so as to not mix evaluating the segmentation and the RF shimming\n",
    "    MPRAGE_shim_monaisegfilename_rfshim=fetch_file_via_name(Processingdir,'*mprage*CV*_pred.nii.gz')\n",
    "    MPRAGE_shim_monaisegfilename_rfshim=MPRAGE_shim_monaisegfilename_rfshim[0]\n",
    "    MPRAGE_shim_monaisegfilename_hard_rfshim=MPRAGE_shim_monaisegfilename_rfshim.split('.')[0]+'_hard.nii.gz'\n",
    "    run_subprocess(f\"sct_maths -i {MPRAGE_shim_monaisegfilename_rfshim} -o {MPRAGE_shim_monaisegfilename_hard_rfshim} -bin 0.5\")\n",
    "    \n",
    "    #And then we need to do manual checking\n",
    "    #And extract the signal intensity    \n",
    "    #GRE_shim_segfilename=fetch_file_via_name(Processingdir,'*gre*_seg.nii.gz')\n",
    "    #GRE_shim_segfilename=GRE_shim_segfilename[0]\n",
    "    #GRE_shim_CSVfile=os.path.join(Processingdir,GRE_shimfile_distcorr.split('/')[-1].split('.')[0]+'_sigint.csv')\n",
    "    #run_subprocess(f\"sct_extract_metric -i {GRE_shimfile_distcorr} -f {GRE_shim_segfilename} -o {GRE_shim_CSVfile} -perslice 1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "96576aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MPRAGE_extract_monai(subjectpath):\n",
    "    MPRAGE_dir=os.path.join(subjectpath,'MPRAGE')   \n",
    "\n",
    "    #We start by finding the directory, and creating the subdirectories for processing and QA\n",
    "    dirname_shimcase=named_subdir(MPRAGE_dir)\n",
    "    [QAdir, Processingdir]=create_QA_and_Processing_dirs_MPRAGE(dirname_shimcase)\n",
    "    \n",
    "    #Fetch filenames\n",
    "    MPRAGE_filenames=fetch_file_via_name(dirname_shimcase,'*mprage*no*nii.gz')\n",
    "    MPRAGE_shimfile_noshim=check_number_of_GRE_scans_distcorr(MPRAGE_filenames)\n",
    "    \n",
    "    MPRAGE_filenames=fetch_file_via_name(dirname_shimcase,'*mprage*rf*nii.gz')\n",
    "    MPRAGE_shimfile_rfshim=check_number_of_GRE_scans_distcorr(MPRAGE_filenames)\n",
    "    #And extract the signal intensity\n",
    "    #Check if there is a corrected file\n",
    "    if not fetch_file_via_name(Processingdir,'*mprage*no*_pred_corr.nii.gz'):\n",
    "        MPRAGE_shim_monaisegfilename_noshim=fetch_file_via_name(Processingdir,'*mprage*no*_pred.nii.gz')\n",
    "    else:\n",
    "        MPRAGE_shim_monaisegfilename_noshim=fetch_file_via_name(Processingdir,'*mprage*no*_pred_corr.nii.gz')\n",
    "    MPRAGE_shim_monaisegfilename_noshim=MPRAGE_shim_monaisegfilename_noshim[0]\n",
    "    \n",
    "    if not fetch_file_via_name(Processingdir,'*mprage*rf*_pred_corr.nii.gz'):\n",
    "        MPRAGE_shim_monaisegfilename_rfshim=fetch_file_via_name(Processingdir,'*mprage*rf*_pred.nii.gz')\n",
    "    else:\n",
    "        MPRAGE_shim_monaisegfilename_rfshim=fetch_file_via_name(Processingdir,'*mprage*rf*_pred_corr.nii.gz')\n",
    "    MPRAGE_shim_monaisegfilename_rfshim=MPRAGE_shim_monaisegfilename_rfshim[0]\n",
    "    \n",
    "    MPRAGE_noshim_CSVfile=os.path.join(Processingdir,MPRAGE_shimfile_noshim.split('/')[-1].split('.')[0]+'_sigint.csv')\n",
    "    run_subprocess(f\"sct_extract_metric -i {MPRAGE_shimfile_noshim} -f {MPRAGE_shim_monaisegfilename_noshim} -o {MPRAGE_noshim_CSVfile} -perslice 1\")\n",
    "   \n",
    "    MPRAGE_rfshim_CSVfile=os.path.join(Processingdir,MPRAGE_shimfile_rfshim.split('/')[-1].split('.')[0]+'_sigint.csv')\n",
    "    run_subprocess(f\"sct_extract_metric -i {MPRAGE_shimfile_rfshim} -f {MPRAGE_shim_monaisegfilename_rfshim} -o {MPRAGE_rfshim_CSVfile} -perslice 1\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "834d1a6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "# Running MONAI GRE segmentation subject by subject to allow for  manual correction\n",
    "subjectpath='/Users/danielpapp/DATA/RF_shimming_project_monai/SubE'\n",
    "shimcasenames=['noshim','patspec','volspec','phaseonly','cvred','target','sareff']\n",
    "for shimcase in range(len(shimcasenames)):\n",
    "    GRE_segment_monai(subjectpath,shimcasenames[shimcase])\n",
    "    print(shimcase)\n",
    "    #B1_map_process(subjectpath,shimcasenames[shimcase])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8cac1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Manual correction was necessary for the following cases:\n",
    "#SubA all OK\n",
    "\n",
    "#SubB CVred\n",
    "#SubB PhaseOnly\n",
    "#SubB Target\n",
    "\n",
    "#SubC CVred\n",
    "#SubC PhaseOnly\n",
    "\n",
    "#SubD CVred\n",
    "#SubD PatSpec\n",
    "#SubD PhaseOnly\n",
    "\n",
    "#Manual correction has to still be performed for:\n",
    "\n",
    "\n",
    "#OVERALL LOW CONFIDENCE IN SUB D MANUAL CORRECTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "848240f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The QA directory already exists! Are you rerunnig the script?\n",
      "The Processing directory already exists! Are you rerunnig the script?\n",
      "0\n",
      "The QA directory already exists! Are you rerunnig the script?\n",
      "The Processing directory already exists! Are you rerunnig the script?\n",
      "1\n",
      "The QA directory already exists! Are you rerunnig the script?\n",
      "The Processing directory already exists! Are you rerunnig the script?\n",
      "2\n",
      "The QA directory already exists! Are you rerunnig the script?\n",
      "The Processing directory already exists! Are you rerunnig the script?\n",
      "3\n",
      "The QA directory already exists! Are you rerunnig the script?\n",
      "The Processing directory already exists! Are you rerunnig the script?\n",
      "4\n",
      "The QA directory already exists! Are you rerunnig the script?\n",
      "The Processing directory already exists! Are you rerunnig the script?\n",
      "5\n",
      "The QA directory already exists! Are you rerunnig the script?\n",
      "The Processing directory already exists! Are you rerunnig the script?\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "# Running MONAI GRE signal extraction subject by subject \n",
    "subjectpath='/Users/danielpapp/DATA/RF_shimming_project_monai/SubC'\n",
    "shimcasenames=['noshim','patspec','volspec','phaseonly','cvred','target','sareff']\n",
    "for shimcase in range(len(shimcasenames)):\n",
    "    GRE_extract_monai(subjectpath,shimcasenames[shimcase])\n",
    "    print(shimcase)\n",
    "    #B1_map_process(subjectpath,shimcasenames[shimcase])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "126dc23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running MONAI MPRAGE segmentation subject by subject \n",
    "subjectpath='/Users/danielpapp/DATA/RF_shimming_project_monai/SubC'\n",
    "MPRAGE_segment_monai(subjectpath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "3d2fad64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The QA directory already exists! Are you rerunnig the script?\n",
      "The Processing directory already exists! Are you rerunnig the script?\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[77], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m shimcasenames\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnoshim\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpatspec\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvolspec\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mphaseonly\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcvred\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msareff\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m shimcase \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(shimcasenames)):\n\u001b[0;32m----> 8\u001b[0m     \u001b[43mB1_map_process_monai\u001b[49m\u001b[43m(\u001b[49m\u001b[43msubjectpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43mshimcasenames\u001b[49m\u001b[43m[\u001b[49m\u001b[43mshimcase\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28mprint\u001b[39m(shimcase)\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;66;03m#B1_map_process(subjectpath,shimcasenames[shimcase])\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[61], line 38\u001b[0m, in \u001b[0;36mB1_map_process_monai\u001b[0;34m(subjectpath, indirname)\u001b[0m\n\u001b[1;32m     36\u001b[0m VoltageAtSocket\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39maround(VoltageAtSocket, decimals\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     37\u001b[0m run_subprocess(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msct_maths -i \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mTFL_FAfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m -div 10 -o \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mTFL_FAfile_nTpV\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 38\u001b[0m \u001b[43mrun_subprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msct_maths -i \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mTFL_FAfile_nTpV\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m -div \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mVoltageAtSocket\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m -o \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mTFL_FAfile_nTpV\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m run_subprocess(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msct_maths -i \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mTFL_FAfile_nTpV\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m -mul 130.492 -o \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mTFL_FAfile_nTpV\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m#Then we coreigster the B1map anatomical to the undistcorrected GRE\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[65], line 50\u001b[0m, in \u001b[0;36mrun_subprocess\u001b[0;34m(cmd)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Wrapper for ``subprocess.run()`` that enables to input ``cmd`` as a full string (easier for debugging).\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;124;03m    cmd (string): full command to be run on the command line\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 50\u001b[0m     \u001b[43msubprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcmd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstdout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPIPE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstderr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSTDOUT\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m subprocess\u001b[38;5;241m.\u001b[39mCalledProcessError \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReturn code: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00merr\u001b[38;5;241m.\u001b[39mreturncode\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/subprocess.py:505\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    503\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Popen(\u001b[38;5;241m*\u001b[39mpopenargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;28;01mas\u001b[39;00m process:\n\u001b[1;32m    504\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 505\u001b[0m         stdout, stderr \u001b[38;5;241m=\u001b[39m \u001b[43mprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcommunicate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    506\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m TimeoutExpired \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    507\u001b[0m         process\u001b[38;5;241m.\u001b[39mkill()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/subprocess.py:1141\u001b[0m, in \u001b[0;36mPopen.communicate\u001b[0;34m(self, input, timeout)\u001b[0m\n\u001b[1;32m   1139\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stdin_write(\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m   1140\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstdout:\n\u001b[0;32m-> 1141\u001b[0m     stdout \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstdout\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1142\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstdout\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m   1143\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# THIS TOTALLY FAILED ON SUB A NOSHIM\n",
    "# TODO CHECK WHAT THE HELL IS GOING ON And processd\n",
    "\n",
    "# Running MONAI B1 processing subject by subject \n",
    "subjectpath='/Users/danielpapp/DATA/RF_shimming_project_monai/SubA'\n",
    "shimcasenames=['noshim','patspec','volspec','phaseonly','cvred','target','sareff']\n",
    "for shimcase in range(len(shimcasenames)):\n",
    "    B1_map_process_monai(subjectpath,shimcasenames[shimcase])\n",
    "    print(shimcase)\n",
    "    #B1_map_process(subjectpath,shimcasenames[shimcase])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be222623",
   "metadata": {},
   "outputs": [],
   "source": [
    "########## OLD CODE STARTS HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "075f6698",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GRE_segment_extract(subjectpath, indirname):\n",
    "    #Define the GRE scan directory the GRE scans\n",
    "    GRE_dir=os.path.join(subjectpath,'GRE_B1')   \n",
    "\n",
    "    #We start by finding the directory, and creating the subdirectories for processing and QA\n",
    "    dirname_shimcase=named_subdir(GRE_dir,indirname)\n",
    "    [QAdir, Processingdir]=create_QA_and_Processing_dirs(dirname_shimcase)\n",
    "    \n",
    "    #We fetch the GRE scans and check that there are only one or two (Wih and without distortion correction)\n",
    "    GRE_filenames=fetch_file_via_name(dirname_shimcase,'*gre*nii.gz')\n",
    "    GRE_shimfile_distcorr=check_number_of_GRE_scans_distcorr(GRE_filenames)\n",
    "    \n",
    "    #Sometimes there is a 0 -0 discrepancy between the QFORM and SFORM for MGH data. To fix this, we use the \n",
    "    # 'set-qform-to-sfrom' option of sct_image\n",
    "    run_subprocess(f\"sct_image -i {GRE_shimfile_distcorr} -set-qform-to-sform\")\n",
    "    \n",
    "    #We can now segment this file\n",
    "    run_subprocess(f\"sct_deepseg_sc -i {GRE_shimfile_distcorr} -c t2s -kernel 2d -centerline svm -ofolder {Processingdir} -qc {QAdir}\")\n",
    "\n",
    "    #And extract the signal intensity    \n",
    "    GRE_shim_segfilename=fetch_file_via_name(Processingdir,'*gre*_seg.nii.gz')\n",
    "    GRE_shim_segfilename=GRE_shim_segfilename[0]\n",
    "    GRE_shim_CSVfile=os.path.join(Processingdir,GRE_shimfile_distcorr.split('/')[-1].split('.')[0]+'_sigint.csv')\n",
    "    run_subprocess(f\"sct_extract_metric -i {GRE_shimfile_distcorr} -f {GRE_shim_segfilename} -o {GRE_shim_CSVfile} -perslice 1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2b29b8ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def B1_map_process(subjectpath,indirname):\n",
    "    #Define the GRE scan directory the GRE scans\n",
    "    GRE_dir=os.path.join(subjectpath,'GRE_B1')   \n",
    "\n",
    "    #We start by finding the directory, and creating the subdirectories for processing and QA\n",
    "    dirname_shimcase=named_subdir(GRE_dir,indirname)\n",
    "    [QAdir_B1, Processingdir_B1]=create_QA_and_Processing_dirs_B1(dirname_shimcase)\n",
    "    \n",
    "    #Next we find the TFL_B1maps anatomical and flip angle maps\n",
    "    TFL_anatfile=find_matching_nii_json_pairs(dirname_shimcase, 'anatomical', 'ImageComments')[0]\n",
    "    TFL_FAfile=find_matching_nii_json_pairs(dirname_shimcase, 'angle map', 'ImageComments')[0]\n",
    "\n",
    "    #We then convert the flip angle map into a nT/V map\n",
    "    # Maths from Kyle Gilbert\n",
    "    #GAMMA = 2.675e8\n",
    "    #B1eff_mag = (AcquiredFA ./ RequestedFA) .* (pi ./ (GAMMA .* 1e-3 .* VoltageAtSocket)); % [T/V]\n",
    "    #B1eff_mag = B1eff_mag .* 1e9; % [T/V] to [nT/V]\n",
    "    # The costants sum up to 130.492, so to convert the B1map to nT/V, it has to be divided by 10 (to get it back into units of FA)\n",
    "    # then multiplied by 130.492 and divided by the VoltageAtSocket\n",
    "\n",
    "    TFL_FAfile_nTpV=os.path.join(Processingdir_B1,TFL_FAfile.split('.')[0].split('/')[-1]+'_nTpV.nii.gz')\n",
    "    TFL_FAfile_jsonfile = TFL_FAfile.replace(\".nii.gz\", \".json\")\n",
    "    RefVol = extract_tx_ref_amp(TFL_FAfile_jsonfile)\n",
    "    VoltageAtSocket = RefVol * 10**-0.095\n",
    "    VoltageAtSocket=np.around(VoltageAtSocket, decimals=2)\n",
    "    run_subprocess(f\"sct_maths -i {TFL_FAfile} -div 10 -o {TFL_FAfile_nTpV}\")\n",
    "    run_subprocess(f\"sct_maths -i {TFL_FAfile_nTpV} -div {VoltageAtSocket} -o {TFL_FAfile_nTpV}\")\n",
    "    run_subprocess(f\"sct_maths -i {TFL_FAfile_nTpV} -mul 130.492 -o {TFL_FAfile_nTpV}\")\n",
    "\n",
    "    #Next we fing the distoriton uncorrected GRE scan, since it is easier to coregister that to the also distortion uncorrected B1 map\n",
    "    GRE_filenames=fetch_file_via_name(dirname_shimcase,'*gre*nii.gz')\n",
    "    GRE_shimfile_nodistcorr=check_number_of_GRE_scans_nodistcorr(GRE_filenames)\n",
    "\n",
    "    #Sometimes there is a 0 -0 discrepancy between the QFORM and SFORM for MGH data. To fix this, we use the \n",
    "    # 'set-qform-to-sfrom' option of sct_image\n",
    "    run_subprocess(f\"sct_image -i {GRE_shimfile_nodistcorr} -set-qform-to-sform\")\n",
    "    \n",
    "    #We can now segment this file\n",
    "    run_subprocess(f\"sct_deepseg_sc -i {GRE_shimfile_nodistcorr} -c t2s -kernel 2d -centerline svm -ofolder {Processingdir_B1} -qc {QAdir_B1}\")\n",
    "    GRE_shim_segfilename=fetch_file_via_name(Processingdir_B1,'*gre*_seg.nii.gz')\n",
    "    GRE_shim_segfilename=GRE_shim_segfilename[0]\n",
    "\n",
    "    #Then we coreigster the B1map anatomical to the undistcorrected GRE\n",
    "    warp_anat_2_B1_fname=os.path.join(Processingdir_B1,GRE_shimfile_nodistcorr.split('.')[0].split('/')[-1]+'_warp2B1.nii.gz')\n",
    "    run_subprocess(f\"sct_register_multimodal -i {TFL_anatfile} -d {GRE_shimfile_nodistcorr} -ofolder {Processingdir_B1} -qc {QAdir_B1} -dseg {GRE_shim_segfilename} -owarp {warp_anat_2_B1_fname}\")\n",
    "\n",
    "    #Apply this transformation to the segmentation we just produced\n",
    "    warped_segname=os.path.join(Processingdir_B1,GRE_shim_segfilename.split('.')[0].split('/')[-1]+'_warped.nii.gz')\n",
    "    run_subprocess(f\"sct_apply_transfo -i {GRE_shim_segfilename} -d {TFL_anatfile} -o {warped_segname} -x nn -w {warp_anat_2_B1_fname}\")\n",
    "\n",
    "    #And finally extact the metric\n",
    "    TFL_FAfile_nTpV_CSV=(TFL_FAfile_nTpV.split('.')[0]+'_SC.csv')\n",
    "    run_subprocess(f\"sct_extract_metric -i {TFL_FAfile_nTpV} -f {warped_segname} -o {TFL_FAfile_nTpV_CSV} -perslice 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "46ec1218",
   "metadata": {},
   "outputs": [],
   "source": [
    "subjectpath='/Users/danielpapp/DATA/RF_shimming_project_clean/SubA'\n",
    "shimcasenames=['noshim','patspec','volspec','phaseonly','cvred','target','sareff']\n",
    "for shimcase in range(len(shimcasenames)):\n",
    "    GRE_segment_extract(subjectpath,shimcasenames[shimcase])\n",
    "    B1_map_process(subjectpath,shimcasenames[shimcase])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9514153f",
   "metadata": {},
   "outputs": [],
   "source": [
    "subjectpath='/Users/danielpapp/DATA/RF_shimming_project_clean/SubB'\n",
    "shimcasenames=['noshim','patspec','volspec','phaseonly','cvred','target','sareff']\n",
    "for shimcase in range(len(shimcasenames)):\n",
    "    GRE_segment_extract(subjectpath,shimcasenames[shimcase])\n",
    "    B1_map_process(subjectpath,shimcasenames[shimcase])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_monai",
   "language": "python",
   "name": "venv_monai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
